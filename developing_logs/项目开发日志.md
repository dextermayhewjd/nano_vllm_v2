# 探索vllm

## 1. 把huggingface 上的模型在本地运行

### 1.1 获取huggingface 上qwen2.5的模型 

[下载模型的代码](../download_data/download_model.py)

[确认能够合理load到模型里](../download_data/verify_data.py)

### 1.2 拆解模型到底是怎么load的  

#### 1.2.1 首先要确定model的结构 也就是每个layer的shape是怎么样的

config.json  这里有每个parameter的大小可以计算出来模型每一个层的大小  
[模型config](../../../Data/models/Qwen/Qwen2.5-7B/config.json)
 

#### 1.2.2 通过model.safetensors.index.json 告诉哪个文件拿tensor权重  
model.safetensors.index.json  
[模型的哪个层 在哪个文件拿权重](../../../Data/models/Qwen/Qwen2.5-7B/model.safetensors.index.json)

比如

```cpp
 "model.layers.0.input_layernorm.weight": "model-00001-of-00004.safetensors",
```

比如这里就显示模型的layer0的layernorm的weight 就在model-00001-of-00004.safetensors这里拿

gpt同意思版本

model.safetensors.index.json 里面的 weight_map 会告诉你：
每个参数 key（比如 model.layers.0.self_attn.q_proj.weight）
存在于哪一个分片文件（比如 model-00001-of-00004.safetensors）

#### 1.2.3 如何读取 某个tensor呢 
[text](../scripts/peek_safetensors.py)

A. 用 safetensors 官方 API（推荐）

```python

with safe_open(str(MODEL_DIR / shard), framework="pt", device="cpu") as f:
    # print("num keys in shard:", len(f.keys()))
    t = f.get_tensor(key)
    # 这里的key就是具体某一个layer的某一个weight组
```

但要注意 这里是通过打开一个safetensor文件来读取每一层的数据  
要避免打开一个读取一层就关闭

逻辑执行顺序是  

1. 打开一个safetensor文件  
2. 读取里面的每一个层  

### 1.3 loader 结合 读取tensor 和 layer层的

[python对象和getattr()](python_对象.md)



## appendix a

### 大模型的整体流程

```bash
token_ids
   ↓
Embedding (model.embed_tokens.weight)
   ↓
Transformer Blocks × N
   ↓
Final Hidden State
   ↓
lm_head.weight
   ↓
logits
   ↓
softmax
   ↓
next token
```

这里先存疑  

### embedding layer 嵌套层

这里是大模型的第一个layer
用于将tokenizer 将文字转化为token的 id 通过hot shot的方式将

```c
[0 0 1]     [1,2,3]     [789]
[0 1 0]  *  [4,5,6] =   [456]   
[1 0 0]     [7,8,9]     [123]
```

之前还有通过先转化为 one shot的tensor之后 构建了再乘 不使用embedding的情况下
使用的是 `self.weights[token_ids]` 直接简化上述流程 直接生成 上面的结果


