# 探索vllm

## 1. 把huggingface 上的模型在本地运行

### 1.1 获取huggingface 上qwen2.5的模型 

[下载模型的代码](../download_data/download_model.py)

[确认能够合理load到模型里](../download_data/verify_data.py)

### 1.2 拆解模型到底是怎么load的  

#### 1.2.1 首先要确定model的结构 也就是每个layer的shape是怎么样的

config.json  这里有每个parameter的大小可以计算出来模型每一个层的大小  
[模型config](../../../Data/models/Qwen/Qwen2.5-7B/config.json)
 

#### 1.2.2 通过model.safetensors.index.json 告诉哪个文件拿tensor权重  
model.safetensors.index.json  
[模型的哪个层 在哪个文件拿权重](../../../Data/models/Qwen/Qwen2.5-7B/model.safetensors.index.json)

比如

```cpp
 "model.layers.0.input_layernorm.weight": "model-00001-of-00004.safetensors",
```

比如这里就显示模型的layer0的layernorm的weight 就在model-00001-of-00004.safetensors这里拿

gpt同意思版本

model.safetensors.index.json 里面的 weight_map 会告诉你：
每个参数 key（比如 model.layers.0.self_attn.q_proj.weight）
存在于哪一个分片文件（比如 model-00001-of-00004.safetensors）

#### 1.2.3 如何读取 某个tensor呢 
[text](../scripts/peek_safetensors.py)

A. 用 safetensors 官方 API（推荐）

```python

with safe_open(str(MODEL_DIR / shard), framework="pt", device="cpu") as f:
    # print("num keys in shard:", len(f.keys()))
    t = f.get_tensor(key)
    # 这里的key就是具体某一个layer的某一个weight组
```

但要注意 这里是通过打开一个safetensor文件来读取每一层的数据  
要避免打开一个读取一层就关闭

逻辑执行顺序是  

1. 打开一个safetensor文件  
2. 读取里面的每一个层  

### 1.2.4 模型的权重keys里面是否包含了模型是怎样构建的呢

1. `"lm_head.weight": "model-00005-of-00005.safetensors",`  
那么模型的第一层必须是 `self.lm_head = `并且= 的右边必须一个包含着weight的layer例如：

```python
class Qwen3ForCausalLM(nn.Module):
   def __init__(
      self,
      config: Qwen3Config
               ) -> None:
      super().__init__() 
      self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)

class Linear(Module):
   def __init__(
        self,
        in_features: int,
        out_features: int,
        bias: bool = True,
        device=None,
        dtype=None,
      ) -> None:
      super().__init__()
      self.in_features = in_features
      self.out_features = out_features
      self.weight = Parameter(
         torch.empty((out_features, in_features), **factory_kwargs)
      )
```

这里的子层linear里面必须有`Parameter`的同时
必须是有`self.weight`

### 1.3 loader 结合 读取tensor 和 layer层的

[python对象和getattr()](python_对象.md)




## appendix a

### 大模型的整体流程

```bash
token_ids
   ↓
Embedding (model.embed_tokens.weight)
   ↓
Transformer Blocks × N
   ↓
Final Hidden State
   ↓
lm_head.weight
   ↓
logits
   ↓
softmax
   ↓
next token
```

这里先存疑  

### embedding layer 嵌套层

这里是大模型的第一个layer
用于将tokenizer 将文字转化为token的 id 通过hot shot的方式将

```c
[0 0 1]     [1,2,3]     [789]
[0 1 0]  *  [4,5,6] =   [456]   
[1 0 0]     [7,8,9]     [123]
```

之前还有通过先转化为 one shot的tensor之后 构建了再乘 不使用embedding的情况下
使用的是 `self.weights[token_ids]` 直接简化上述流程 直接生成 上面的结果


qwen3 是使用的 GQA 
实现了之后
验证过程中遇到的几个问题是精度

一开始先用简单的token和huggingface自带的实现
实现ref用来对照[dump reference](../nanovllm/scripts/dump_hf_reference.py)

随后在确保其余都没有问题的情况下看了下
test_load model 看每一层的精度[检查精度](../nanovllm/scripts/test_load_model.py)

在这之前先要保证能够load到model里面 其次要保证尺寸一样[loader](../nanovllm/utils/loader.py)

碰到的问题是在一开始的attention那里就错了
报错原因是rope的实现和论文不一样 huggingface采用了 更加适合5

1. 做 KV cache v1（连续内存版，不分页）+ 单请求逐 token decode。
2. 做 prefill/decode 两阶段调度（先单卡、先正确）。
3. 把 KV 改成 paged（block table + block manager）。
4. 再上 PagedAttention kernel（先正确性版，再性能版）。
5. 再考虑 FlashAttention 优化 prefill（你现在用 SDPA 已经可能自动走 flash 内
   核）。
6. 最后做分布式（TP/PP），否则会把单卡 correctness 问题放大。